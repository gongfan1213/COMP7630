以下是逐句对照的中英文翻译：

===== Page 1 =====  
COMP7630 – Web Intelligence and its Applications  
COMP7630 – 网络智能及其应用  

Singular Value Decomposition  
奇异值分解  

Valentino Santucci  
瓦伦蒂诺·桑图奇  

(valentino.santucci@unistrapg.it)  
（邮箱：valentino.santucci@unistrapg.it）  

===== Page 2 =====  
# Singular Value Decomposition (SVD)  
# 奇异值分解（SVD）  

- Eigendecomposition does not work with rectangular matrices (and also with non-diagonalizable matrices)  
- 特征分解不适用于矩形矩阵（以及不可对角化的矩阵）  

- SVD generalizes EigenDecomposition on rectangular matrices (to be precise: on any kind of matrix)  
- SVD将特征分解推广至矩形矩阵（准确地说：适用于任何类型的矩阵）  

- It is also possible to describe PCA by using SVD, though we will not do it.  
- 虽然本课程不展开，但SVD也可用于描述主成分分析（PCA）。  
  - The trick is to work directly with data-table X and not with \( X'X \) as seen before.  
  - 关键在于直接操作数据表X，而非之前提到的\( X'X \)。  
  - ScikitLearn uses the SVD implementation, though it is mathematically equivalent to what we have seen!  
  - ScikitLearn采用SVD实现，但其数学本质与前述方法等价。  

===== Page 3 =====  
Singular Value Decomposition (SVD)  
奇异值分解（SVD）  

Another way to factorize a matrix into singular vectors and single values.  
将矩阵分解为奇异向量和奇异值的另一种方法。  

Every real matrix (even not a square) has a SVD.  
任何实数矩阵（包括非方阵）均存在SVD分解。  

Singular value decomposition:  
奇异值分解公式：  
\( A = U \Sigma V^T \)  

U and V are orthogonal matrices.  
U和V是正交矩阵。  

\(\Sigma\) is diagonal (but not necessarily square) and the elements along the diagonal are the singular values.  
\(\Sigma\)为对角矩阵（不一定是方阵），对角线元素为奇异值。  

U is left-singular vector and V is right-singular vector.  
U是左奇异向量，V是右奇异向量。  

===== Page 4 =====  
Some properties of the SVD  
SVD的性质  

U columns are the eigenvectors of \( A^TA \) (which is symmetric by definition)  
U的列是\( A^TA \)的特征向量（\( A^TA \)对称）。  
They are called "left singular vectors" of A  
称为A的“左奇异向量”。  

V rows are the eigenvectors of \( AA^T \)  
V的行是\( AA^T \)的特征向量。  
They are called "right singular vectors" of A  
称为A的“右奇异向量”。  

The non-zero diagonal values in \(\Sigma\) are the square-root of the eigenvalues of both \( A^TA \) and \( AA^T \) (which are equal)  
\(\Sigma\)的非零对角元素是\( A^TA \)与\( AA^T \)（二者相等）特征值的平方根。  
They are called "singular values" of A  
称为A的“奇异值”。  

===== Page 5 =====  
Singular Value Decomposition (SVD)  
奇异值分解（SVD）  

\[A_{m \times n} = \mathbf{U}_{m \times m} \times \Sigma_{m \times n} \times V^{T}_{n \times n}\]  
（当 \( m < n \) 时）  

\[A_{m \times n} = \mathbf{U}_{m \times m} \times \Sigma_{m \times n} \times V^{T}_{n \times n}\]  
（当 \( m > n \) 时）  

===== Page 6 =====  
Truncated SVD  
截断SVD  

\[A_{m \times n} \approx \tilde{A}_k = U_{m \times k} \Sigma_{k \times k} V_{k \times n}^T\]  
保留前k个最大奇异值的近似分解。  

===== Page 7 =====  
Truncated SVD  
截断SVD  

Keep only the \( k \) largest singular values.  
仅保留前k个最大的奇异值。  

\[A \approx \tilde{A}_k = U_k \Sigma_k V_k^T\]  
\(\tilde{A}_k \in \mathbb{R}^{m \times n}, \quad U_k \in \mathbb{R}^{m \times k}, \quad \Sigma_k \in \mathbb{R}^{k \times k}, \quad V_k \in \mathbb{R}^{k \times n}\)  

It can be shown that this gives the minimum value for the Frobenius norm of \(\|A - \tilde{A}_k\|_F\)  
可证明该近似使\(\|A - \tilde{A}_k\|_F\)的Frobenius范数最小化。  

===== Page 8 =====  
# Truncated SVD  
# 截断SVD  

- It is important to note that:  
- 需注意：  
  - We reduce the number of columns in the "left-matrix" \( U \)  
  - 左矩阵\( U \)的列数减少  
  - We reduce the number of rows in the "right-matrix" \( V \)  
  - 右矩阵\( V \)的行数减少  
  - But \( \tilde{A} \) has the same shape of \( A \)  
  - 但\( \tilde{A} \)与\( A \)的维度相同  

- Anyway, \( \tilde{A} \) has a smaller rank than \( A \)  
- 无论如何，\( \tilde{A} \)的秩低于\( A \)  
  (recall: the rank of a matrix is the maximum number of columns which are linearly independent to each other)  
  （秩是矩阵中线性无关列的最大数量）  

- Why this may be useful?  
- 为何有用？  
  - \( A \) can be interpreted as a data-matrix containing noise  
  - \( A \)可视为含噪声的数据矩阵  
  - \( \tilde{A} \) is a denoised version of \( A \)  
  - \( \tilde{A} \)是去噪后的版本  
  - \( U_k \) rows are denoised/reduced representations of \( A \) rows  
  - \( U_k \)的行是\( A \)行的去噪/降维表示  
  - \( V_k^T \) columns are denoised/reduced representations of \( A \) columns  
  - \( V_k^T \)的列是\( A \)列的去噪/降维表示  

===== Page 9 =====  
# Another perspective on SVD  
# SVD的另一视角  

\[U \Sigma V^T x = \sum_k \sigma_k u_k v_k^T x\]  
Hence \( U \Sigma V^T = \sum_k \sigma_k u_k v_k^T \).  
因此SVD可表示为秩1矩阵的加权和。  

- The SVD decomposition can be rewritten as a sum of rank-1 matrices: those obtained by the outer product between the \( k \)-th column of \( U \) and the \( k \)-th row of \( V^T \), weighted by \( k \)-th singular value.  
- SVD分解可重写为秩1矩阵的和：由\( U \)的第k列与\( V^T \)的第k行外积，再乘以第k个奇异值构成。  
  - Recall: the outer product of two vectors returns a rank-1 matrix.  
  - 注：两向量的外积生成秩1矩阵。  

- So, the Truncated SVD acts as removing the terms with smaller weights in the summation!!!  
- 截断SVD即移除求和中小权重的项！  
  - (If you are familiar with it, it is a sort of "discrete Fourier transform")  
  - （类似于“离散傅里叶变换”中的截断思想）  

===== Page 10 =====  
SVD in Python  
Python中的SVD实现  

===== Page 11 =====  
Truncated SVD in Python  
Python中的截断SVD  

代码示例（输出略）：  
- 逐步展示k=1,2,3时的重构矩阵\( X_k \)与误差范数。  
- 当k=3时，重构误差极小（3.68e-15），几乎完全还原原矩阵。  

（注：代码部分因格式原因未逐句翻译，保留关键注释）
