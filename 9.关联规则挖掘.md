===== Page 1 =====  
COMP7630 - Web Intelligence and its Applications  
COMP7630 - 网络智能及其应用  

Association Rules  
关联规则  

Valentino Santucci  
瓦伦蒂诺·桑图奇  

(valentino.santucci@unistrapg.it)  
（电子邮件：valentino.santucci@unistrapg.it）  

===== Page 2 =====  
# Mining Association Rules  
# 挖掘关联规则  

- Mining association rules allows to discover regularities in data  
- 挖掘关联规则可以发现数据中的规律性  

- Objective: to find all co-occurrence relationships among data items  
- 目标：发现数据项之间的所有共现关系  

- Classic application: market basket data analysis, which aims to discover how items purchased by customers in a supermarket (or a store) are associated  
- 经典应用：购物篮数据分析，旨在发现顾客在超市（或商店）中购买的商品之间的关联  

===== Page 3 =====  
# Example Association Rule  
# 关联规则示例  

Cheese → Beer  
奶酪 → 啤酒  

\[ \text{[support = 10%, confidence = 80%]} \]  
\[ \text{[支持度 = 10%，置信度 = 80%]} \]  

- (Support) 10% of the customers buy Cheese and Beer together  
- （支持度）10%的顾客同时购买奶酪和啤酒  

- (Confidence) Customers who buy Cheese also buy Beer 80% of the times  
- （置信度）购买奶酪的顾客中有80%也会购买啤酒  

- Support and Confidence are two measures of rule strength  
- 支持度和置信度是衡量规则强度的两个指标  

===== Page 4 =====  
Possible applications in the web  
网络中的可能应用  

Purchases patterns in e-commerce websites  
电子商务网站中的购买模式  

Word co-occurrence relationships  
词语共现关系  

Hashtag suggestion in social networks  
社交媒体中的话题标签建议  

Web usage patterns  
网络使用模式  

===== Page 5 =====  
# Definitions  
# 定义  

- \( I = \{t_1, t_2, ..., t_m\} \) is the universe set of items  
- \( I = \{t_1, t_2, ..., t_m\} \) 是所有物品的全集  

- \( T = \{t_1, t_2, ..., t_n\} \) is the set (or database) of transactions  
- \( T = \{t_1, t_2, ..., t_n\} \) 是事务的集合（或数据库）  

- \( t_i \subseteq I \), i.e., each transaction is a subset of items  
- \( t_i \subseteq I \)，即每个事务是物品的一个子集  

- An association rule is an implication of the form  
- 关联规则是一种形式的蕴含关系  

  \[ X \to Y, \text{ where: } X \subset I, Y \subset I, X \cap Y = \emptyset \]  
  \[ X \to Y, \text{ 其中: } X \subset I, Y \subset I, X \cap Y = \emptyset \]  

- \( X \) and \( Y \) are called itemsets  
- \( X \) 和 \( Y \) 被称为项集  

===== Page 6 =====  
# An example  
# 示例  

**Example 1:** We want to analyze how the items sold in a supermarket are related to one another. \( I \) is the set of all items sold in the supermarket. A transaction is simply a set of items purchased in a basket by a customer. For example, a transaction may be:  
**示例1：** 我们想分析超市中销售的商品之间的关系。\( I \) 是超市中销售的所有商品的集合。一个事务就是顾客在购物篮中购买的一组商品。例如，一个事务可能是：  

\[\{ \text{Beef, Chicken, Cheese} \},\]  
\[\{ \text{牛肉, 鸡肉, 奶酪} \},\]  

which means that a customer purchased three items in a basket, Beef, Chicken, and Cheese. An association rule may be:  
这表示顾客在购物篮中购买了三种商品：牛肉、鸡肉和奶酪。一个关联规则可能是：  

\[\text{Beef, Chicken} \rightarrow \text{Cheese},\]  
\[\text{牛肉, 鸡肉} \rightarrow \text{奶酪},\]  

where \(\{ \text{Beef, Chicken} \}\) is \( X \) and \(\{ \text{Cheese} \}\) is \( Y \). For simplicity, brackets “\(\{\)” and “\(\}\)” are usually omitted in transactions and rules.  
其中 \(\{ \text{牛肉, 鸡肉} \}\) 是 \( X \)，\(\{ \text{奶酪} \}\) 是 \( Y \)。为简化起见，事务和规则中的花括号“\(\{\)”和“\(\}\)”通常被省略。  

===== Page 7 =====  
# Some other definitions  
# 其他定义  

- The transaction \( t_i \in T \) contains the itemset \( X \subseteq I \) iff \( X \subseteq t_i \)  
- 事务 \( t_i \in T \) 包含项集 \( X \subseteq I \) 当且仅当 \( X \subseteq t_i \)  

- In that case, we also say that \( X \) covers \( t_i \)  
- 在这种情况下，我们也说 \( X \) 覆盖了 \( t_i \)  

- The support count of \( X \) in \( T \) is the number of transactions in \( T \) that contain \( X \). The support count is denoted by \( X.count \)  
- \( X \) 在 \( T \) 中的支持计数是 \( T \) 中包含 \( X \) 的事务数量。支持计数记为 \( X.count \)  

===== Page 8 =====  
Support and Confidence of a rule  
规则的支持度和置信度  

• Given an association rule 𝑋→𝑌, we define its support and confidence:  
• 给定关联规则 𝑋→𝑌，我们定义其支持度和置信度：  

===== Page 9 =====  
# Relationships with probabilities  
# 与概率的关系  

- Support is the percentage of transactions in \( T \) that contains \( X \cup Y \), i.e., that contains both \( X \) and \( Y \).  
- 支持度是 \( T \) 中包含 \( X \cup Y \)（即同时包含 \( X \) 和 \( Y \)）的事务的百分比。  

- Support is an estimate of \( P(X \cup Y) \) or, better, it estimates \( P(X \text{ and } Y) = P(X, Y) \)  
- 支持度是对 \( P(X \cup Y) \) 的估计，更准确地说，它估计 \( P(X \text{ 和 } Y) = P(X, Y) \)。  

- Confidence is the percentage of transactions in \( T \) containing \( X \) that also contain \( Y \)  
- 置信度是 \( T \) 中包含 \( X \) 的事务中同时包含 \( Y \) 的百分比。  

- Confidence is an estimate of \( P(Y|X) \)  
- 置信度是对 \( P(Y|X) \) 的估计。  

- RECALL conditional probability definition:  
- 回顾条件概率的定义：  

\[P(Y|X) = \frac{P(X,Y)}{P(X)} \Rightarrow P(X,Y) = P(Y|X)P(X) = P(X|Y)P(Y)\]  
\[P(Y|X) = \frac{P(X,Y)}{P(X)} \Rightarrow P(X,Y) = P(Y|X)P(X) = P(X|Y)P(Y)\]  

===== Page 10 =====  
# Why Support and Confidence?  
# 为什么需要支持度和置信度？  

- Support is a useful measure because if it is too low, the rule may just occur due to chance. Furthermore, in a business environment, a rule covering too few cases (or transactions) may not be useful because it does not make business sense to act on such a rule (not profitable).  
- 支持度是一个有用的度量，因为如果支持度过低，规则可能只是偶然出现。此外，在商业环境中，覆盖太少案例（或事务）的规则可能没有用，因为基于这种规则采取行动没有商业意义（不盈利）。  

- Confidence determines the predictability of the rule. If the confidence of a rule is too low, one cannot reliably infer or predict Y from X. A rule with low predictability is of limited use.  
- 置信度决定了规则的可预测性。如果规则的置信度过低，就无法可靠地从X推断或预测Y。可预测性低的规则用途有限。  

===== Page 11 =====  
# Mining of Association Rules  
# 关联规则的挖掘  

Given a transaction set \( T \), the problem of mining association rules is to discover all association rules in \( T \) that have support and confidence greater than or equal to the user-specified minimum support (denoted by **minsup**) and minimum confidence (denoted by **minconf**).  
给定事务集 \( T \)，挖掘关联规则的问题是发现 \( T \) 中所有支持度和置信度大于或等于用户指定的最小支持度（记为 **minsup**）和最小置信度（记为 **minconf**）的关联规则。  

===== Page 12 =====  
# Example  
# 示例  

**Input**  
**输入**  

t₁:  
Beef, Chicken, Milk  
牛肉, 鸡肉, 牛奶  

t₂:  
Beef, Cheese  
牛肉, 奶酪  

t₃:  
Cheese, Boots  
奶酪, 靴子  

t₄:  
Beef, Chicken, Cheese  
牛肉, 鸡肉, 奶酪  

t₅:  
Beef, Chicken, Clothes, Cheese, Milk  
牛肉, 鸡肉, 衣服, 奶酪, 牛奶  

t₆:  
Chicken, Clothes, Milk  
鸡肉, 衣服, 牛奶  

t₇:  
Chicken, Milk, Clothes  
鸡肉, 牛奶, 衣服  

**minsup = 30%**  
**最小支持度 = 30%**  

**minconf = 80%**  
**最小置信度 = 80%**  

The following association rules are valid:  
以下关联规则是有效的：  

**Expected Output**  
**预期输出**  

Rule 1:  
Chicken, Clothes → Milk  
鸡肉, 衣服 → 牛奶  

Rule 2:  
Clothes, Milk → Chicken  
衣服, 牛奶 → 鸡肉  

Rule 3:  
Clothes → Milk, Chicken  
衣服 → 牛奶, 鸡肉  

[sup = 3/7, conf = 3/3]  
[支持度 = 3/7, 置信度 = 3/3]  

[sup = 3/7, conf = 3/3]  
[支持度 = 3/7, 置信度 = 3/3]  

[sup = 3/7, conf = 3/3]  
[支持度 = 3/7, 置信度 = 3/3]  

===== Page 13 =====  
# Apriori Algorithm  
# Apriori算法  

- Apriori is one of the best known algorithm for mining association rules  
- Apriori是挖掘关联规则最著名的算法之一  

- The Apriori algorithm has been proposed in *Agrawal, Rakesh, and Ramakrishnan Srikant. "Fast algorithms for mining association rules." Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994.*  
- Apriori算法由*Agrawal, Rakesh和Ramakrishnan Srikant*在论文《Fast algorithms for mining association rules》中提出，发表于1994年VLDB会议。  

===== Page 14 =====  
# Apriori algorithm: how it works  
# Apriori算法的工作原理  

- Apriori works in two steps:  
- Apriori算法分为两步：  

1. **Generate all frequent itemsets:** A frequent itemset is an itemset that has transaction support above minsup  
1. **生成所有频繁项集：** 频繁项集是支持度高于最小支持度的项集  

2. **Generate all confident association rules from the frequent itemsets:**  
   A confident association rule is a rule with confidence above minconf  
2. **从频繁项集中生成所有置信关联规则：**  
   置信关联规则是置信度高于最小置信度的规则  

- Note: in practical applications, sometimes only the first step is enough to reach the objective at hand  
- 注意：在实际应用中，有时仅第一步就足以实现目标  

===== Page 15 =====  
# Apriori step 1: generate all frequent itemsets  
# Apriori第一步：生成所有频繁项集  

## Downward Closure Property: If an itemset has minimum support, then every non-empty subset of this itemset also has minimum support.  
## 向下闭包性质：如果一个项集具有最小支持度，那么它的每个非空子集也具有最小支持度。  

---  

### Algorithm Apriori(T)  
### Apriori算法(T)  

1. \( C_1 \leftarrow \text{init-pass}(T); \)  
   // the first pass over \( T \)  
   // 第一次扫描 \( T \)  

2. \( F_i \leftarrow \{ f | f \in C_k, f \text{count}/n \geq \text{minsup} \}; \)  
   // \( n \) is the no. of transactions in \( T \)  
   // \( n \) 是 \( T \) 中的事务数量  

3. **for** \((k = 2; F_{i-1} \neq \emptyset; k++)\) **do**  
   // subsequent passes over \( T \)  
   // 后续扫描 \( T \)  

4. \( C_k \leftarrow \text{candidate-gen}(F_{i-1}); \)  

5. **for** each transaction \( t \in T \) **do**  
   // scan the data once  
   // 扫描数据一次  

6. **for** each candidate \( c \in C_k \) **do**  

7. **if** \( c \) is contained in \( t \) **then**  

8. \( c.\text{count}+t; \)  

9. **endfor**  

10. \( F_k \leftarrow \{ c \in C_k | c.\text{count}/n \geq \text{minsup} \} \)  

11. **endfor**  

12. **return** \( F \leftarrow \bigcup_k F_k; \)  

---  

### Function candidate-gen(\( F_{i-1} \))  
### 候选生成函数(\( F_{i-1} \))  

1. \( C_k \leftarrow \emptyset; \)  
   // initialize the set of candidates  
   // 初始化候选集  

2. **forall** \( f_1, f_2 \in F_{i-1} \)  
   // find all pairs of frequent itemsets  
   // 找到所有频繁项集对  

3. with \( f_1 = \{ i_1, \ldots, i_{k-2}, i_{k-1} \} \)  
   // that differ only in the last item  
   // 仅在最后一项不同  

4. and \( f_2 = \{ i_1, \ldots, i_{k-3}, i_{k-1} \} \)  

5. and \( i_{k-1} < i_{k-1} \) **do**  
    // according to the lexicographic order  
    // 根据字典序  

6. \( c \leftarrow \{ i_1, \ldots, i_{k-1}, i_{k-1} \}; \)  
    // join the two itemsets \( f_1 \) and \( f_2 \)  
    // 连接两个项集 \( f_1 \) 和 \( f_2 \)  

7. \( C_k \leftarrow C_k \cup \{ c \}; \)  
    // add the new itemset \( c \) to the candidates  
    // 将新项集 \( c \) 加入候选集  

8. **for** each \((k-1)\)-subset \( s \) of \( c \) **do**  

9. **if** \((s \notin F_{i-1})\) **then**  

10. delete \( c \) from \( C_k; \)  

11. **endfor**  

12. **endfor**  

13. **return** \( C_k; \)  
    // delete \( c \) from the candidates  
    // 从候选集中删除 \( c \)  
    // return the generated candidates  
    // 返回生成的候选集  

---  

Note: Not necessary to load the whole data into memory before processing, only one transaction must reside in memory.  
注意：处理前无需将所有数据加载到内存中，只需一个事务驻留在内存中即可。  

===== Page 16 =====  
# Apriori step 1 ... an "animated" example  
# Apriori第一步...一个“动态”示例  

- We have the following dataset of transactions  
- 我们有以下事务数据集  

| TID    | Items |  
|---|---|  
| T1    | 1 3 4   |  
| T2    | 2 3 5   |  
| T3    | 1 2 3 5 |  
| T4    | 2 5    |  
| T5    | 1 3 5   |  

and we assume minsup=40%, i.e., at least 2 transactions need to be covered, so in the following I simplify support to be the number of transactions covered by a given itemset  
假设最小支持度为40%，即至少需要覆盖2个事务，因此在以下内容中，我将支持度简化为给定项集覆盖的事务数量  

===== Page 17 =====  
Apriori step 1 … an "animated" example  
Apriori第一步...一个“动态”示例  

• Find candidate itemsets of length 1  
• 生成长度为1的候选项集  

===== Page 18 =====  
# Apriori step 1 ... an "animated" example  
# Apriori第一步...一个“动态”示例  

- Filter-out candidates with support<minsup and obtain F1 (frequent itemsets of length 1)  
- 过滤掉支持度小于最小支持度的候选，得到F1（长度为1的频繁项集）  

| Itemset | Support |  
|---|---|  
| (1)    | 3    |  
| (2)    | 3    |  
| (3)    | 4    |  
| (4)    | 1    |  
| (5)    | 4    |  

| Itemset | Support |  
|---|---|  
| (1)    | 3    |  
| (2)    | 3    |  
| (3)    | 4    |  
| (5)    | 4    |  

===== Page 19 =====  
Apriori step 1 … an "animated" example  
Apriori第一步...一个“动态”示例  

• Create candidate of length 2 and filter-out low-support itemsets  
• 生成长度为2的候选并过滤掉低支持度的项集  

===== Page 20 =====  
# Apriori step 1 ... an "animated" example  
# Apriori第一步...一个“动态”示例  

- Create candidates of length 3  
- 生成长度为3的候选  

| TID    | Items    |  
|---|---|  
| T1    | 1 3 4   |  
| T2    | 2 3 5   |  
| T3    | 1 2 3 5   |  
| T4    | 2 5    |  
| T5    | 1 3 5   |  

| Itemset    | In F27 |  
|---|---|  
| {1,2,3}, {1,2}, {1,3}, {2,3} | NO    |  
| {1,2,5}, {1,2}, {1,5}, {2,5} | NO    |  
| {1,3,5},{1,5}, {1,3}, {3,5} | YES    |  
| {2,3,5}, {2,3}, {2,5}, {3,5} | YES    |  

===== Page 21 =====  
# Apriori step 1 ... an "animated" example  
# Apriori第一步...一个“动态”示例  

- Filter-out C3 to generate frequent itemsets of length 3  
- 过滤掉C3生成长度为3的频繁项集  

| TID    | Items    |  
|---|---|  
| T1    | 1 3 4    |  
| T2    | 2 3 5    |  
| T3    | 1 2 3 5    |  
| T4    | 2 5    |  
| T5    | 1 3 5    |  

| Itemset   | Support    |  
|---|---|  
| {1,3,5}    | 2    |  
| {2,3,5}    | 2    |  

===== Page 22 =====  
# Apriori step 1 ... an "animated" example  
# Apriori第一步...一个“动态”示例  

- Create candidates of length 4  
- 生成长度为4的候选  

| TID | Items |  
|---|---|  
| T1   | 1 3 4    |  
| T2   | 2 3 5    |  
| T3   | 1 2 3 5    |  
| T4   | 2 5    |  
| T5   | 1 3 5    |  

| Itemset | Support |  
|---|---|  
| {1,3,5} | 2    |  
| {2,3,5} | 2    |  

| Itemset | Support |  
|---|---|  
| {1,2,3,5} | 1    |  

Since all candidates in C4 are filtered-out, the algorithm terminates and its result is the union of F1, F2, and F3.  
由于C4中的所有候选都被过滤掉，算法终止，其结果是F1、F2和F3的并集。  

===== Page 23 =====  
# Apriori step 2: generate association rules  
# Apriori第二步：生成关联规则  

- Simple strategy:  
   for every frequent itemset \( f \) and for each subset \( \alpha \subset f \):  
   output the rule  
   \[   (f - \alpha) \to \alpha, \text{ if }\]  
   \[   confidence = \frac{f.count}{(f - \alpha).count} \geq minconf,\]  
   where \( f.count \) (or \((f - \alpha).count\)) is the support count of \( f \) (or \((f - \alpha)\)), which can be easily obtained from the supports computed in step 1.  
- 简单策略：  
   对于每个频繁项集 \( f \) 和每个子集 \( \alpha \subset f \)：  
   输出规则  
   \[   (f - \alpha) \to \alpha, \text{ 如果 }\]  
   \[   置信度 = \frac{f.count}{(f - \alpha).count} \geq minconf,\]  
   其中 \( f.count \)（或 \((f - \alpha).count\)）是 \( f \)（或 \((f - \alpha)\)）的支持计数，可以从第一步计算的支持度中轻松获得。  

===== Page 24 =====  
# Apriori step 2... a more efficient strategy  
# Apriori第二步...更高效的策略  

- In order to design an efficient strategy, let's observe that the support count of \( f \) does not change as \(\alpha\) changes  
- 为了设计一个高效的策略，我们注意到 \( f \) 的支持计数不会随着 \(\alpha\) 的变化而变化  

- Therefore, if \((f - \alpha) \to \alpha\) is valid, then all the rules of the form \((f - \alpha_{sub}) \to \alpha_{sub}\), with \(\alpha_{sub} \subset \alpha\), are valid  
- 因此，如果 \((f - \alpha) \to \alpha\) 是有效的，那么所有形式为 \((f - \alpha_{sub}) \to \alpha_{sub}\)（其中 \(\alpha_{sub} \subset \alpha\)）的规则也是有效的  

- Example: if \(A, B \to C, D\) is valid, then also \(A, B, C \to D\) and \(A, B, D \to C\) are valid  
- 例如：如果 \(A, B \to C, D\) 是有效的，那么 \(A, B, C \to D\) 和 \(A, B, D \to C\) 也是有效的  

- Therefore, an efficient algorithm very similar to "candidate-gen" (seen some slides before) can be devised  
- 因此，可以设计一个与“候选生成”（前面幻灯片中提到的）非常相似的高效算法  

===== Page 25 =====  
# Lift: another measure for association rules  
# 提升度：关联规则的另一种度量  

- \( \text{Lift}(X \to Y) = \frac{\text{Confidence}(X \to Y)}{Y.\text{count}/n} \)  
- \( \text{提升度}(X \to Y) = \frac{\text{置信度}(X \to Y)}{Y.\text{count}/n} \)  

- **Lift** is an estimate of  
  \[  \frac{P(X,Y)}{P(Y)P(X)} \]  
- **提升度** 是对以下值的估计  
  \[  \frac{P(X,Y)}{P(Y)P(X)} \]  

- **Therefore** *lift* is a kind of "correlation" between \( X \) and \( Y \)  
- **因此** *提升度* 是 \( X \) 和 \( Y \) 之间的一种“相关性”  

- Informally speaking, a high *lift* indicates that the importance of an association rule is not just a coincidence  
- 通俗地说，高*提升度*表明关联规则的重要性不仅仅是巧合  

===== Page 26 =====  
One more example  
另一个示例  

minsup = 0.5  
最小支持度 = 0.5  

minconf = 0.8  
最小置信度 = 0.8  

maxlen = 2  
最大长度 = 2  

INPUT  
输入  

===== Page 27 =====  
One more example  
另一个示例  

minsup = 0.5  
最小支持度 = 0.5  

minconf = 0.8  
最小置信度 = 0.8  

maxlen = 2  
最大长度 = 2  

INPUT  
输入  

Support(A) = 3/5 = 0.6  
支持度(A) = 3/5 = 0.6  

Support(B) = 3/5 = 0.6  
支持度(B) = 3/5 = 0.6  

Support(C) = 2/5 = 0.4  
支持度(C) = 2/5 = 0.4  

Support(D) = 4/5 = 0.8  
支持度(D) = 4/5 = 0.8  

Support(E) = 3/5 = 0.6  
支持度(E) = 3/5 = 0.6  

Support(A,B) = 2/5 = 0.4  
支持度(A,B) = 2/5 = 0.4  

Support(A,D) = 2/5 = 0.4  
支持度(A,D) = 2/5 = 0.4  

Support(A,E) = 1/5 = 0.2  
支持度(A,E) = 1/5 = 0.2  

Support(B,D) = 2/5 = 0.4  
支持度(B,D) = 2/5 = 0.4  

Support(B,E) = 1/5 = 0.2  
支持度(B,E) = 1/5 = 0.2  

Support(D,E) = 3/5 = 0.6  
支持度(D,E) = 3/5 = 0.6  

Support Computation  
支持度计算  

===== Page 28 =====  
One more example  
另一个示例  

minsup = 0.5  
最小支持度 = 0.5  

minconf = 0.8  
最小置信度 = 0.8  

maxlen = 2  
最大长度 = 2  

INPUT  
输入  

Support(A) = 3/5 = 0.6  
支持度(A) = 3/5 = 0.6  

Support(B) = 3/5 = 0.6  
支持度(B) = 3/5 = 0.6  

Support(C) = 2/5 = 0.4  
支持度(C) = 2/5 = 0.4  

Support(D) = 4/5 = 0.8  
支持度(D) = 4/5 = 0.8  

Support(E) = 3/5 = 0.6  
支持度(E) = 3/5 = 0.6  

Support(A,B) = 2/5 = 0.4  
支持度(A,B) = 2/5 = 0.4  

Support(A,D) = 2/5 = 0.4  
支持度(A,D) = 2/5 = 0.4  

Support(A,E) = 1/5 = 0.2  
支持度(A,E) = 1/5 = 0.2  

Support(B,D) = 2/5 = 0.4  
支持度(B,D) = 2/5 = 0.4  

Support(B,E) = 1/5 = 0.2  
支持度(B,E) = 1/5 = 0.2  

Support(D,E) = 3/5 = 0.6  
支持度(D,E) = 3/5 = 0.6  

Support Computation  
支持度计算  

Confidence(D->E) = 0.6 / 0.8 = 0.75  
置信度(D->E) = 0.6 / 0.8 = 0.75  

Confidence(E->D) = 0.6 / 0.6 = 1  
置信度(E->D) = 0.6 / 0.6 = 1  

Confidence Computation  
置信度计算  

===== Page 29 =====  
One more example  
另一个示例  

minsup = 0.5  
最小支持度 = 0.5  

minconf = 0.8  
最小置信度 = 0.8  

maxlen = 2  
最大长度 = 2  

INPUT  
输入  

Support(A) = 3/5 = 0.6  
支持度(A) = 3/5 = 0.6  

Support(B) = 3/5 = 0.6  
支持度(B) = 3/5 = 0.6  

Support(C) = 2/5 = 0.4  
支持度(C) = 2/5 = 0.4  

Support(D) = 4/5 = 0.8  
支持度(D) = 4/5 = 0.8  

Support(E) = 3/5 = 0.6  
支持度(E) = 3/5 = 0.6  

Support(A,B) = 2/5 = 0.4  
支持度(A,B) = 2/5 = 0.4  

Support(A,D) = 2/5 = 0.4  
支持度(A,D) = 2/5 = 0.4  

Support(A,E) = 1/5 = 0.2  
支持度(A,E) = 1/5 = 0.2  

Support(B,D) = 2/5 = 0.4  
支持度(B,D) = 2/5 = 0.4  

Support(B,E) = 1/5 = 0.2  
支持度(B,E) = 1/5 = 0.2  

Support(D,E) = 3/5 = 0.6  
支持度(D,E) = 3/5 = 0.6  

Support Computation  
支持度计算  

Confidence(D->E) = 0.6 / 0.8 = 0.75  
置信度(D->E) = 0.6 / 0.8 = 0.75  

Confidence(E->D) = 0.6 / 0.6 = 1  
置信度(E->D) = 0.6 / 0.6 = 1  

Confidence Computation  
置信度计算  

Lift(E->D) = 1 / 0.8 = 1.25  
提升度(E->D) = 1 / 0.8 = 1.25  

Lift Computation  
提升度计算  

===== Page 30 =====  
# Practical applications other than market basket  
# 购物篮以外的实际应用  

- Any text documents can be seen as a transaction, where each distinct word/lemma is an item (duplicate words are removed)  
- 任何文本文档都可以被视为事务，其中每个不同的单词/词元是一个项（重复的单词被移除）  

- Same as before, but consider windows of words of a given maximum length  
- 与之前相同，但考虑给定最大长度的单词窗口  

- Relational tables with categorical values are easily seen as transactions  
- 具有分类值的关系表很容易被视为事务  

- If numerical values are present, we need to discretize them to categories  
  - Example:  
    \[    \begin{align*}  
    Temperature & \leq 0^\circ \\  
    0^\circ & < Temperature \leq 15^\circ \\  
    15^\circ & < Temperature \leq 25^\circ \\  
    \end{align*}\]  
    \[    \begin{align*}  
    => "very cold" \\  
    => "cold" \\  
    => "warm" \\  
    => "hot"  
    \end{align*}\]  
- 如果存在数值，我们需要将其离散化为类别  
  - 例如：  
    \[    \begin{align*}  
    温度 & \leq 0^\circ \\  
    0^\circ & < 温度 \leq 15^\circ \\  
    15^\circ & < 温度 \leq 25^\circ \\  
    \end{align*}\]  
    \[    \begin{align*}  
    => "非常冷" \\  
    => "冷" \\  
    => "温暖" \\  
    => "热"  
    \end{align*}\]  

===== Page 31 =====  
# Some more (complex) extensions  
# 更多（复杂）扩展  

- Rare items problem: distinctive items may be interesting at different minimum supports.  
- 稀有项问题：独特项可能在不同的最小支持度下具有意义。  

- Class labels: transactions may be labeled by classes and users may be interested in targeted rules, i.e., rules whose right-hand-side is a class label.  
- 类别标签：事务可能被类别标记，用户可能对目标规则感兴趣，即右侧是类别标签的规则。  

- Sequential patterns: association rules do not consider the order of transactions, so sequences of items (and not simply itemsets) can be considered.  
- 序列模式：关联规则不考虑事务的顺序，因此可以考虑项的序列（而不仅仅是项集）。  

- All these cases have been formally defined and specific mining algorithms (which are extensions of the Apriori algorithm) have been proposed  
- 所有这些情况都已正式定义，并提出了特定的挖掘算法（这些算法是Apriori算法的扩展）。  

===== Page 32 =====  
Hands-on with Python  
Python实践  

• Requirements:  
• 要求：  

• Python (>=3.8, but it may be ok also an older version)  
• Python（>=3.8，但旧版本也可以）  

• pip install mlxtend  
• 安装mlxtend库  

• Two examples in arules.zip:  
• arules.zip中的两个示例：  

• basket.py (mining of rules for a simple market basket)  
• basket.py（简单购物篮的规则挖掘）  

• recipes.py (mining of rules for a dataset of recipes… where recipes are lists formed by cuisine_type + list of ingredients)  
• recipes.py（食谱数据集的规则挖掘…其中食谱是由菜系类型和配料列表组成的列表）  

===== Page 33 =====  
# References  
# 参考文献  

- **Liu, Bing.** *Web data mining: exploring hyperlinks, contents, and usage data. Berlin: springer, 2011. Chapter 2.*  
- **刘兵.** *《网络数据挖掘：探索超链接、内容和用法数据》. 柏林: Springer, 2011. 第2章.*  

- **Agrawal, Rakesh, and Ramakrishnan Srikant.** "Fast algorithms for mining association rules." Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994.  
  https://courses.cs.duke.edu/compsci516/spring16/Papers/AssociationRuleMining.pdf  
- **Agrawal, Rakesh和Ramakrishnan Srikant.** "Fast algorithms for mining association rules." 第20届国际大型数据库
