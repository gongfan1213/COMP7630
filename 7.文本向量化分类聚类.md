以下是逐句对照的中英文翻译：

===== 第1页 =====  
COMP7630 – Web Intelligence and its Applications  
COMP7630 – 网络智能及其应用  

Text Classification and Clustering with Scikit Learn  
使用Scikit Learn进行文本分类与聚类  

Valentino Santucci  
瓦伦蒂诺·桑图奇  
(valentino.santucci@unistrapg.it)  
(valentino.santucci@unistrapg.it)  

===== 第2页 =====  
# Outline  
# 大纲  

- Text Vectorization  
- 文本向量化  
- Text Classification  
- 文本分类  
- Text Clustering  
- 文本聚类  

===== 第3页 =====  
# Text Vectorization  
# 文本向量化  

- In order to apply common Machine Learning algorithms to texts, we need to vectorize a text  
- 为了将常见机器学习算法应用于文本，我们需要对文本进行向量化  

- Vectorizing a text = transform a text into a fixed-length vector of numerical features  
- 文本向量化 = 将文本转换为固定长度的数值特征向量  

- We already know how to vectorize a text by using Spacy & SBERT  
- 我们已经了解如何使用Spacy和SBERT进行文本向量化  

- Classification and clustering techniques here described apply to all the vectorization methodologies  
- 本文描述的文本分类与聚类技术适用于所有向量化方法  

===== 第4页 =====  
# Build a data matrix \( X \) from a corpus using Spacy  
# 使用Spacy从语料库构建数据矩阵 \( X \)  

## fn [74]: import spacy  
## 代码[74]: 导入spacy库  

## fn [75]: import numpy as np  
## 代码[75]: 导入numpy库  

## fn [76]: nlp = spacy.load('en_core.web.md')  
## 代码[76]: 加载Spacy的英文模型'en_core.web.md'  

## fn [77]: texts = ['Hong Kong is a beautiful city!', ...' 'Bruce Lee has from Hong Kong', ...' 'Hong Kong and Macau are two Chinese special administrative regions', ...' 'Macau has a very beautiful historical center!', ...' 'Hong Kong and Macau are two cities', ...' 'Perugia is a city as well']  
## 代码[77]: 定义文本列表  

## fn [78]: docs = [ nlp(text) for text in texts ]  
## 代码[78]: 对每个文本进行Spacy处理  

## fn [79]: X = np.vstack([ doc.vector for doc in docs ])  
## 代码[79]: 将文本向量堆叠为数据矩阵  

## fn [80]: X.shape  
## 代码[80]: 查看矩阵形状  
## Out[80]: (6, 380)  
## 输出[80]: (6行, 380列)  

## fn [81]: X  
## 代码[81]: 查看矩阵内容  
## Out[81]:  
**array([[-1.0696642 , 0.6536115 , -2.9217427 , ..., 0.11435284, -2.347317 , 3.4459348 ]),**  
- [-3.1138649 , -3.4459454 , -0.4697681 , ..., -0.81481665, -0.9796772 , 3.4629667 ],  
- [-3.6748297 , -2.9744825 , -0.65786394 , ..., -3.63582 , -2.5489378 , 1.9917101 ],  
- [-0.38297462 , 1.39113 , -2.1896926 , ..., -0.5993681 , -1.8713462 , 1.3179818 ],  
- [-4.9277993 , -3.9634918 , -0.84754276 , ..., -3.5127575 , -2.4722978 , 1.6814859 ]  
- [-2.864188 , 2.0463782 , -3.8127083 , ..., -1.1261166 , -1.373985 , 2.9202565 ], dtype=float32)  

---  
## Data Matrix  
## 数据矩阵  
A numpy matrix where each row represents a text and each column is a feature (obtained by using some methodology – word2vec vectors in this example).  
一个numpy矩阵，每行代表一个文本，每列代表一个特征（本例中使用word2vec向量生成）。  

This kind of data matrix is used by Scikit Learn for classification, clustering and preprocessing.  
Scikit Learn使用此类数据矩阵进行分类、聚类和预处理。  

Note for Spacy word-embeddings  
关于Spacy词嵌入的注意事项  
The vector of a Spacy container is the average of the vectors of its tokens, also stop-word tokens!!!  
Spacy容器的向量是其所有词符向量的平均值（包括停用词词符！！！）  

It may be useful to implement the average of non-stop-word tokens by our own.  
可能需要自行实现仅对非停用词词符取平均。  

===== 第5页 =====  
... or using a SBERT Sentence Transformer  
... 或使用SBERT句子转换器  

In [2]: from sentence_transformers import SentenceTransformer  
代码[2]: 导入SentenceTransformer库  

In [3]: model = SentenceTransformer('all-MiniLM-LG-v2')  
代码[3]: 加载预训练模型'all-MiniLM-LG-v2'  

In [4]: sentences = [ 'Hong Kong is a beautiful city!',  
    'Bruce Lee was from Hong Kong',  
    'Hong Kong and Macau are two Chinese special administrative regions',  
    'Macau has a very beautiful historical center!',  
    'Hong Kong and Macau are two cities',  
    'Perugia is a city as well' ]  
代码[4]: 定义句子列表  

In [5]: X = model.encode(sentences)  
代码[5]: 对句子进行编码  

In [6]: X.shape  
代码[6]: 查看编码后形状  
Out[6]: (6, 384)  
输出[6]: (6行, 384列)  

In [7]: X  
代码[7]: 查看编码结果  
Out[7]:  
array([ [ 0.11758427, 0.05357994, 0.09608648, ..., -0.03971702,  
    -0.01584323, 0.05885128],  
    [-0.09223547, 0.10798477, -0.0278198 , ..., -0.86692163,  
    -0.08186112, 0.06139071],  
    [ 0.11354191, 0.01777782, 0.06273731, ..., 0.81355424,  
    -0.05585783, 0.05616575],  
    [ 0.12832547, 0.04275572, -0.01268561, ..., -0.05665375,  
    -0.08824164, 0.03712672],  
    [ 0.1231629, 0.02269989, 0.02983097, ..., -0.02046551,  
    -0.02853159, 0.06652158],  
    [ 0.08116611, -0.00793275, -0.0166357 , ..., 0.02632581,  
    0.01981751, -0.034713611], dtype=float32)  

Notes for SBERT  
关于SBERT的说明  
The embeddings are no more average of words' embeddings, but SBERT models truncate long texts, so it may be helpful to first segment the sentences, then encode every sentence separately and average them (also using weighted average if some sentences are more important).  
SBERT生成的嵌入不再是词嵌入的平均值，但SBERT模型会截断长文本，因此建议先分割句子，分别编码后再取平均（若某些句子更重要，可使用加权平均）。  

Since SBERT embeddings are vectors of entire sentences, stopwords removal is usually not necessary.  
由于SBERT生成的是整句的向量，通常无需移除停用词。  

===== 第6页 =====  
Normalization  
归一化  

Normalizing a vector = dividing the vector by its (Euclidean/L2) norm  
向量归一化 = 将向量除以其（欧几里得/L2）范数  

A normalized vector has norm 1  
归一化后的向量范数为1  

Geometrically, the normalization acts as a projection of the vector towards the closer point in the surface of a hyper-sphere with radius 1  
几何意义上，归一化将向量投影到半径为1的超球面最近点  

===== 第7页 =====  
Normalization is useful with vectorized texts  
归一化对向量化文本非常有用  

Often, vectorized texts are further elaborated by means of algorithms which make use of the Euclidean distance (let think for example to K-means for clustering or K-nearest-neighbor for classification)  
向量化文本常需基于欧氏距离的算法进一步处理（例如K均值聚类或K近邻分类）  

However, we know that a good distance function for vectorized texts is the cosine distance (or similarity)  
但文本向量的最佳距离函数是余弦距离（或相似度）  

It has been observed that it works fine also when comparing texts of very different lengths  
实践表明，该函数在比较长度差异大的文本时效果良好  

Normalization helps because Euclidean distance between normalized vectors is (almost) the same as cosine distance between unnormalized vectors  
归一化的意义在于：归一化向量间的欧氏距离（几乎）等同于未归一化向量的余弦距离  

The "almost" means that the distances are not really the same but they have the same ranking  
"几乎"指距离值虽不同，但排序一致  

Final remark: if computational time is not an issue, the best option is always: try both!  
最后建议：若计算时间允许，最佳做法是两种方法都尝试！  

===== 第8页 =====  
# Outline  
# 大纲  

- Text Vectorization  
- 文本向量化  
- Text Classification  
- 文本分类  
- Text Clustering  
- 文本聚类  

===== 第9页 =====  
# Let install Scikit Learn library  
# 安装Scikit Learn库  

- If you have created a webintelligence environment (highly suggested), then activate it with:  
- 若已创建webintelligence环境（强烈建议），请激活：  
  conda activate webintelligence  

- Then:  
- 然后执行：  
  pip install scikit-learn  

===== 第10页 =====  
# Scikit Learn Classifiers  
# Scikit Learn分类器  

- Logistic Regression  
- 逻辑回归  
  LogisticRegression  

- Support Vector Machine  
- 支持向量机  
  SVC  

- Random Forest  
- 随机森林  
  RandomForestClassifier  

- Multi-layer Perceptron  
- 多层感知机  
  MLPClassifier  

- K-Nearest-Neighbor  
- K近邻  
  KNeighborsClassifier  

... and many others. See the following link for a complete guide to classification with Scikit Learn:  
... 及其他。完整分类指南见：  
https://scikit-learn.org/stable/supervised-learning.html#supervised-learning  

===== 第11页 =====  
# Common interface for all SKLearn Classifiers  
# Scikit Learn分类器的通用接口  

- A constructor where to set classifier hyperparameters  
- 构造函数：用于设置分类器超参数  

- .fit(X,y) method which requires:  
- .fit(X,y) 方法需提供：  
  - the vectorized data-matrix \( X \) where any row is a record/sample  
  - 向量化数据矩阵 \( X \)，每行为一个样本  
  - a vector of integer values \( y \) such that \( y[i] \) is the label of record \( X[i] \)  
  - 整型标签向量 \( y \)，\( y[i] \) 对应 \( X[i] \) 的标签  

- Executing .fit(X,y) means "training", so after its execution (which may require time), a model has been learned and it is now possible to make prediction on unseen records  
- 执行.fit(X,y)即"训练"，完成后可对未知样本进行预测  

- .predict(X_new) method returns the predicted labels of all the rows/samples in the data-matrix \( X \_new \)  
- .predict(X_new) 方法返回 \( X \_new \) 中所有样本的预测标签  

- IMPORTANT: always assess performance of a model generated by a classifier on data samples not used for training the model!!!  
- 重要：始终在未参与训练的样本上评估分类器性能！！！  

===== 第12页 =====  
# Training and Test Sets + Cross Validation  
# 训练集/测试集 + 交叉验证  

- **Scikit Learn has useful functions and classes for:**  
- **Scikit Learn提供以下功能：**  

  - splitting a dataset into training set + test set  
  - 数据集划分为训练集和测试集  
    train_test_split  

  - setting up cross-validation and quickly execute it  
  - 快速设置并执行交叉验证  
    RepeatedStratifiedKFold  
    cross_val_score  

  - calculate a variety of metrics  
  - 计算多种评估指标  
    accuracy_score, ...  

===== 第13页 =====  
# What is cross validation  
# 什么是交叉验证  

## 4-fold validation (k=4)  
## 4折交叉验证 (k=4)  

| Fold 1 | Testing set | Training set |  
| 第1折 | 测试集      | 训练集       |  
| Fold 2 | Training set | **Testing set** | Training set |  
| 第2折 | 训练集      | **测试集**   | 训练集       |  
| Fold 3 | Training set | **Testing set** | Training set |  
| 第3折 | 训练集      | **测试集**   | 训练集       |  
| Fold 4 | Training set | **Testing set** |  
| 第4折 | 训练集      | **测试集**   |  

0%  
25%  
50%  
75%  
100%  

Any single fold is an experiment (train + test), so there is a score (e.g. accuracy) for any fold.  
每折均为一次独立实验（训练+测试），因此每折均有评分（如准确率）。  

"Stratified" means that the original distribution of labels is maintained (approximately) in all the training and test sets.  
"分层"指所有训练集和测试集均（近似）保持原始标签分布。  

===== 第14页 =====  
# Hands on Classification with SKLearn  
# Scikit Learn分类实战  

- Classifying 200 texts selected from the 20newsgroups dataset which is publicly available at [http://qwone.com/~jason/20Newsgroups/](https://qwone.com/~jason/20Newsgroups/)  
- 对来自20newsgroups数据集的200篇文本进行分类（数据集公开地址）  

- They are messages obtained from thematic newsgroups and two themes are selected as labels: auto and space  
- 文本来自主题新闻组，选取两个主题作为标签：汽车(auto)与太空(space)  

- See the files in  
- 示例文件见：  
  classification_clustering_examples.zip  

===== 第15页 =====  
Again ... a very important concept for a correct experimentation in classification  
再次强调...分类实验中的重要原则  

- Never use test set information for training, so also in preprocessing!!!  
- 切勿在训练中使用测试集信息，预处理阶段也需遵守！！！  

- This principle applies also to all the preprocessing operations that work "vertically" on the dataset, i.e. that modify entire columns of the data matrix  
- 该原则同样适用于所有"纵向"预处理操作（即修改数据矩阵整列的操作）  

- Examples?  
- 例如？  
  - Standardization  
  - 标准化  
  - Dimensionality reduction with PCA  
  - PCA降维  

- In these cases you need to fit (.fit) the preprocessing operation only on the training set, then apply it (.transform) to the test/validation set  
- 此类操作需仅在训练集上拟合(.fit)，再对测试集/验证集应用(.transform)  

- What about normalization?  
- 归一化如何处理？  
  - Normalization works on the rows and does not involve columns, so it does not pose any problem when validation or cross-validation is considered.  
  - 归一化针对行操作且不涉及列，因此在验证或交叉验证中无需特殊处理。  

===== 第16页 =====  
# How to translate this into Scikit Learn? (1/2)  
# 如何在Scikit Learn中实现？(1/2)  

- As an example, let's consider Standardization as preprocessing step and Logistic Regression as classifier  
- 以标准化为预处理步骤、逻辑回归为分类器为例  

- Validation case (train_test_split)  
- 验证场景（train_test_split）  

| In [24] | #%* is a data matrix already loaded, while 'y' is its corresponding labels vector.    |  
| 代码[24] | #%* 为已加载的数据矩阵，'y'为其对应标签向量 |  
| In [25] | X_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, shuffle=True, stratify=y)    |  
| 代码[25] | 划分训练集/测试集（测试集占比20%，打乱数据并分层） |  
| In [26] | scaler = StandardScaler()    |  
| 代码[26] | 初始化标准化器 |  
| In [27] | scaler.fit(X_train)    | Fitthe scaler only on the training set, then transform the training set    |  
| 代码[27] | 仅在训练集上拟合标准化器 |  
| Out[27] | StandardScaler()    |    |  
| 输出[27] | 标准化器对象 |  
| In [28] | X_train = scaler.transform(X_train)    |  
| 代码[28] | 对训练集应用标准化 |  
| In [29] | clf = LogisticRegression()    |  
| 代码[29] | 初始化逻辑回归分类器 |  
| In [30] | clf.fit(X_train, y_train)    | Run the training (on the training set only)    |  
| 代码[30] | 训练分类器（仅使用训练集） |  
| Out[30] | LogisticRegression()    |    |  
| 输出[30] | 逻辑回归对象 |  
| In [31] | X_test = scaler.transform(X_test)    | Before predicting on the test set, transform the test set using the same scaler fitted before    |  
| 代码[31] | 预测前用已拟合的标准化器处理测试集 |  
| In [32] | y_pred = clf.predict(X_test)    |  
| 代码[32] | 预测测试集标签 |  
| In [33] | acc = accuracy_score(y_test, y_pred)    |  
| 代码[33] | 计算准确率 |  
| In [34] | acc    |  
| 代码[34] | 查看准确率 |  
| Out[34] | 0.825    |  
| 输出[34] | 0.825 |  

===== 第17页 =====  
# How to translate this into Scikit Learn? (2/2)  
# 如何在Scikit Learn中实现？(2/2)  

- Cross Validation case (cross_val_score)  
- 交叉验证场景（cross_val_score）  

---  

### In [56]: from sklearn.pipeline import Pipeline  
### 代码[56]: 导入Pipeline类  

---  

A Sklearn pipeline is a sequence of zero or more preprocessors + (optionally) one classifier as very last element.  
Scikit Learn的Pipeline是预处理步骤（0或多个）与分类器（最后一步）的组合。  

---  

### In [57]: #'X' is a data matrix already loaded, while 'y' is its corresponding labels vector.  
### 代码[57]: #'X'为已加载数据矩阵，'y'为对应标签向量  

---  

### In [58]: clf_plus_preprocessing = Pipeline( steps = [ { 'standardization', StandardScaler() },  { 'classification', LogisticRegression() } ] )  
### 代码[58]: 创建含标准化和逻辑回归的Pipeline  

---  

### In [59]: cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=5)  
### 代码[59]: 设置10次重复的5折分层交叉验证  

---  

### In [60]: accuracies = cross_val_score(clf_plus_preprocessing, X, y, cv=cv, scoring='accuracy')  
### 代码[60]: 执行交叉验证并计算准确率  

---  

### In [61]: accuracies.mean()  
### 代码[61]: 计算平均准确率  

---  

### Out[61]: 0.9725  
### 输出[61]: 0.9725  

---  

Run cross validation using the defined pipeline.  
使用定义好的Pipeline运行交叉验证。  
The implementation of the Pipeline object ensures that no data of the validation set is used for preprocessing!!!  
Pipeline的实现确保验证集数据不会用于预处理！！！  

===== 第18页 =====  
# One remark about PCA on Scikit Learn  
# 关于Scikit Learn中PCA的说明  

class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', n_oversamples=10, power_iteration_normalizer='auto', random_state=None) [source]  
PCA类参数定义  

**Parameters:**  
**参数:**  
n_components : int, float or 'mle', default=None  
保留的主成分数量：整数、浮点数或'mle'，默认为None  

Number of components to keep. If n_components is not set all components are kept:  
未设置时保留所有成分：  

n_components == min(n_samples, n_features)  
n_components等于样本数与特征数的最小值  

If n_components == 'mle' and svd_solver == 'full', Minka's MLE is used to guess the dimension. Use of  
若设为'mle'且svd_solver为'full'，则使用Minka的MLE算法估计维度。  

n_components == 'mle' will interpret svd_solver == 'auto' as svd_solver == 'full'.  
此时'svd_solver'的'auto'选项会被视为'full'。  

If @ < n_components < 1 and svd_solver == 'full', select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.  
若n_components为(0,1)区间浮点数且svd_solver为'full'，则选择解释方差超过该百分比的最小成分数。  

If svd_solver == 'arpack', the number of components must be strictly less than the minimum of n_features and n_samples.  
若svd_solver为'arpack'，成分数必须严格小于样本数与特征数的最小值。  

Hence, the None case results in:  
因此，None参数会导致：  

n_components == min(n_samples, n_features) - 1  
n_components等于min(n_samples, n_features) - 1  

---  
https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html  

===== 第19页 =====  
# Outline  
# 大纲  

- Text Vectorization  
- 文本向量化  
- Text Classification  
- 文本分类  
- Text Clustering  
- 文本聚类  

===== 第20页 =====  
# Clustering  
# 聚类  

- Clustering is an Unsupervised Machine Learning task, where the goal is to group data points based on similarity, aiming to identify inherent patterns or structures within the data.  
- 聚类是无监督机器学习任务，目标是根据相似性分组数据点，以识别数据内在模式或结构。  
  - Input = a dataset of unlabeled instances  
  - 输入 = 无标签样本的数据集  
  - Output = a cluster ID for each instance  
  - 输出 = 每个样本的聚类ID  

- Also Dimensionality Reduction (PCA or SVD) are Unsupervised Machine Learning  
- 降维（PCA或SVD）也属于无监督机器学习  
  - Input = a dataset of unlabeled instances  
  - 输入 = 无标签样本的数据集  
  - Output = new numerical representations for the instances  
  - 输出 = 样本的新数值表示  

===== 第21页 =====  
# Agglomerative Hierarchical Clustering  
# 凝聚层次聚类  

- Clustering is usually based on some definition of distance  
- 聚类通常基于某种距离定义  

- The most simple way to cluster instances is:  
- 最简单的聚类方法步骤：  
  1. Calculate the distances between all the pairs of instances  
  1. 计算所有样本对的距离  
  2. Initially set singleton clusters (each instance defines a cluster, with only itself)  
  2. 初始化为单样本聚类（每个样本自成一类）  
  3. Merge the two closer clusters  
  3. 合并距离最近的两个类  
  4. Go back to step 3 until a termination condition is satisfied  
  4. 重复步骤3直到满足终止条件  
  (e.g.: pre-established number of clusters reached, some quality measure is reached, etc.)  
  （例如：达到预设聚类数量或质量指标）  

- Pros: it is agnostic to the distance measure, it allows to plot a dendrogram  
- 优点：可适配任意距离度量，支持绘制树状图  

- Cons: it requires a large amount of distance computations (at least \( O(n^2) \))  
- 缺点：需大量距离计算（至少 \( O(n^2) \)）  

===== 第22页 =====  
# K-Means  
# K均值算法  

- K-Means is an unsupervised machine learning algorithm used for clustering data into \( k \) groups (or clusters)  
- K均值是无监督学习算法，用于将数据划分为 \( k \) 个簇  

- K-Means requires in input:  
- K均值输入要求：  
  - a data-matrix \( X \) whose rows are vectorized objects that have to be partitioned  
  - 数据矩阵 \( X \)，每行为待分组的向量化对象  
  - the number of clusters \( k \)  
  - 聚类数量 \( k \)  
  - some hyperparameters (such as the initial guess of \( k \) cluster's centroids)  
  - 超参数（如初始 \( k \) 个质心的猜测）  

- K-Means returns in output:  
- K均值输出结果：  
  - a vector \( y \) such that \( y[i] \) contains the id of the cluster decided for object \( X[i] \)  
  - 向量 \( y \)，\( y[i] \) 表示 \( X[i] \) 所属簇ID  
  - a goodness measure (called *inertia*) of the clusterization performed  
  - 聚类质量度量（称为*惯性*）  

===== 第23页 =====  
# How K-Means works  
# K均值工作原理  

1. The algorithm starts with an arbitrary initial setting of \( k \) centroids  
1. 算法随机初始化 \( k \) 个质心  

2. Each data point is assigned to the closest centroid  
2. 每个数据点分配到最近的质心  

3. The centroids are then recomputed as the mean of all the data points assigned to it  
3. 质心更新为其所属数据点的均值  

4. This process repeats until the centroids no longer move or a maximum number of iterations is reached  
4. 重复上述步骤直到质心稳定或达到最大迭代次数  

- Pros: it is more efficient than Hierarchical Aggl. Clust. (each iteration, only require \( k*n \) distance calculations)  
- 优点：比层次聚类高效（每次迭代仅需 \( k*n \) 次距离计算）  

- Cons: it is implicitly based on the Euclidean distance (due to centroids calculations) ... but normalization of the input practically mitigates this issue  
- 缺点：隐含依赖欧氏距离（因质心计算），但输入归一化可缓解此问题  

===== 第24页 =====  
# Initial Setting for the Centroids?  
# 如何初始化质心？  

- The final clustering depends on the initial setting of the centroids  
- 最终聚类结果受初始质心影响  

- In step 1, the \( k \) centroids are usually randomly initialized, but other methodologies can be adopted. In fact, the initial setting of the centroids is an hyperparameter of the \( K \)-Means which can also be adjusted by using some strategy.  
- 通常随机初始化 \( k \) 个质心，但也可采用其他策略。实际上，初始质心设置是K均值的超参数，可通过策略调整。  

- \( K \)-Means also compute an inertia measure for a given centroids' setting  
- K均值会计算给定质心设置下的惯性值  

- The final inertia returned by \( K \)-Means can be used to measure the goodness of different initial centroids' settings.  
- 最终惯性值可用于评估不同初始质心的优劣  

- What is the inertia of a centroid setting? Sum of squared distances of samples to their closest cluster centroid (i.e. the cluster to which the sample is assigned by \( K \)-Means).  
- 惯性定义：样本到其所属质心的平方距离之和  

- However, note that inertia is meaningful only for comparing clusterings with the same \( k \).  
- 注意：惯性仅适用于相同 \( k \) 值的聚类比较  
  - Indeed, as \( k \) increases, the inertia decreases, because there are more clusters, so each cluster has fewer points, so the points are closer to their cluster centers.  
  - 实际上，\( k \) 增大时惯性会减小，因为更多簇意味着每个簇包含更少点，点更接近其质心  

===== 第25页 =====  
# Number of Clusters in K-Means  
# K均值中的聚类数量  

- The number of clusters (\(k\)) must be specified in advance  
- 必须预先指定聚类数量 \( k \)  

- If you do not have any extra-information which guide you in setting \(k\)?  
- 若无额外信息指导 \( k \) 的设置？  

- Silhouette Score measures the similarity of an observation to its own cluster compared to other clusters  
- 轮廓系数衡量样本与同簇和其他簇的相似度  

- Silhouette scores of each sample can be averaged  
- 可计算所有样本轮廓系数的平均值  

- Run K-Means with different values of \(k\), compute the average silhouette scores in each case, then select the one with the largest silhouette score.  
- 尝试不同 \( k \) 值运行K均值，计算各情况下的平均轮廓系数，选择得分最高的 \( k \)  

===== 第26页 =====  
# Silhouette Score (of a sample)  
# 轮廓系数（样本级）  

\[s_i = \frac{b_i - a_i}{\max(b_i, a_i)}\]  
其中：  

- \( a_i \) is the mean intra-cluster distance of sample \( i \)  
- \( a_i \) 是样本 \( i \) 与同簇其他样本的平均距离  
  (i.e. the mean distance between \( i \) and samples assigned to its same cluster)  

- \( b_i \) is the mean nearest cluster distance  
- \( b_i \) 是样本 \( i \) 与最近簇的平均距离  
  (i.e. the mean distance between \( i \) and samples assigned to nearest cluster)  

- \( s \) lies in [-1,+1] and need to be maximized  
- \( s \) 取值范围[-1,+1]，需最大化  

\[a_i = \frac{1}{|C_i| - 1} \sum_{j \in C_i, i \neq j} d(i,j)\]  

\[b_i = \min_{k \neq i} \frac{1}{|C_k|} \sum_{j \in C_k} d(i,j)\]  

===== 第27页 =====  
# Hands on Clustering with SKLearn  
# Scikit Learn聚类实战  

- Cluster 200 texts selected from the 20newsgroups dataset which is publicly available at http://qwone.com/~jason/20Newsgroups/  
- 对来自20newsgroups数据集的200篇文本进行聚类（数据集公开地址）  

- We do not use label informations for clustering!  
- 聚类过程中不使用标签信息！  

- See the files in  
- 示例文件见：  
  `classification_clustering_examples.zip`  

===== 第28页 =====  
References  
参考文献  

Manuals, tutorials and examples available in the Scikit Learn website  
Scikit Learn官网提供的手册、教程与示例  
https://scikit-learn.org/stable/
